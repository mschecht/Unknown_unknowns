group_by(sample_ID) %>%
unique
# Join two lists of non-redundant GOS data
GOS_contexual_data_1 <- inner_join(GOS.character, GOS.numeric, "sample_ID")
# remove unsequenced samples
GOS_unsequenced <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/GOS_samples_unsequenced.txt", col_names = TRUE) %>%
rename(sample_ID = "label")
# How many unsequenced samples are found in the set?
GOS_unsequenced$label %in% GOS_contexual_data_1$sample_ID %>% length()
# If there are, filter them out
GOS_contexual_data_2 <- GOS_contexual_data_1 %>% anti_join(GOS_unsequenced)
# How many of these samples are found in Chiaras set?
GOS_contexual_data_2$sample_ID %in% contex_real$X1
# If there are remove them
# as Chiara about how to do this
GOS_contexual_data_3 <- GOS_contexual_data_2 %>% anti_join(contex_real %>% filter(grepl("GS", sample_ID)))
GOS_contexual_data_3 <- GOS_contexual_data_2 %>%
mutate(project = "GOS") %>%
filter(!is.na(latitude)) # removed a strange row with a label without data
# Final check to see if your set matches Chiara's set
GOS_contexual_data_3$sample_ID %in% contex_real$sample_ID
#############################################################
#####################################
# Malaspina data upload
#####################################
# Upload
Malaspina_contexual_data <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/Malaspina_Metadata_20170703_ed.txt", col_names = TRUE) %>%
rename(sample_ID = "Code MP####", latitude = Lat, longitude = Long, depth = "DEPTH Megafile", temperature = "Temp", salinity = "Sal (PSU)") %>%
dplyr::select(-Project) %>%
mutate(project = "Malaspina")
Malaspina_header_corrections <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/malaspina_header_corrections.txt", col_names = TRUE) %>% dplyr::select("contex", "orf") %>% drop_na()
# Add "MP" to every sample_ID
Malaspina_contexual_data$sample_ID <- paste0("MP", Malaspina_contexual_data$sample_ID)
# Headers that do not need to be corrected
Malaspina_contexual_data_1 <- Malaspina_contexual_data %>% filter(sample_ID %in% contex_real$sample_ID)
# Headers that need to be changed
Malaspina_contexual_data_2 <- Malaspina_contexual_data %>% filter(!sample_ID %in% contex_real$sample_ID)
# Replace headers that need to be changed with headers that match ORFs
Malaspina_contexual_data_3 <- Malaspina_contexual_data_2 %>% mutate(sample_ID = plyr::mapvalues(x = sample_ID, from = Malaspina_header_corrections$contex, to = Malaspina_header_corrections$orf))
# Bind rows headers that did not need to be changed and headers that were corrected
Malaspina_contexual_data_4 <- bind_rows(Malaspina_contexual_data_1, Malaspina_contexual_data_3)
# Upload and clean TARA
TARA_contexual_data <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/TARA_metadata_pangaea_ed.txt", col_names = TRUE) %>%
mutate(project = "TARA") %>%
rename(sample_ID = "ena_read_no", latitude = latitude_verb, longitude = longitude_verb, depth = "depth_verb", temperature = "Tpot [degreesC]", salinity = "Sal", oxygen = "OXYGEN_1 [micromol/kg]", sampleID_tara = SampleID)
TARA_contexual_data_1 <- TARA_contexual_data %>% filter(sample_ID %in% contex_real$sample_ID)
# load Halpern data for TARA
load("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/Halpern/halpern_data.Rda")
TARA_contexual_data_1 <- TARA_contexual_data_1 %>% left_join(out_rescaled_2013 %>% rename("sample_ID" = label))
# There should still be 242 samples
TARA_contexual_data_1 %>% dim()
# There should by only 63 prok + SRF samples with Halpern data
TARA_contexual_data_1 %>% select(sample_ID, global_cumul_impact_2013_all_layers_2013_5kms_mean) %>% drop_na() %>% dim()
# Upload and clean TARA
TARA_contexual_data <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/TARA_metadata_pangaea_ed.txt", col_names = TRUE) %>%
mutate(project = "TARA") %>%
rename(sample_ID = "ena_read_no", depth_category = "event",latitude = latitude_verb, longitude = longitude_verb, depth = "depth_verb", temperature = "Tpot [degreesC]", salinity = "Sal", oxygen = "OXYGEN_1 [micromol/kg]", sampleID_tara = SampleID)
# Upload and clean TARA
TARA_contexual_data <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/TARA_metadata_pangaea_ed.txt", col_names = TRUE) %>%
mutate(project = "TARA") %>%
rename(sample_ID = "ena_read_no", depth_category = "Event",latitude = latitude_verb, longitude = longitude_verb, depth = "depth_verb", temperature = "Tpot [degreesC]", salinity = "Sal", oxygen = "OXYGEN_1 [micromol/kg]", sampleID_tara = SampleID)
# Upload and clean TARA
TARA_contexual_data <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/TARA_metadata_pangaea_ed.txt", col_names = TRUE) %>%
mutate(project = "TARA") %>%
rename(sample_ID = "ena_read_no", depth_category = "Event",latitude = latitude_verb, longitude = longitude_verb, depth = "depth_verb", temperature = "Tpot [degreesC]", salinity = "Sal", oxygen = "OXYGEN_1 [micromol/kg]", sampleID_tara = SampleID)
TARA_contexual_data_1 <- TARA_contexual_data %>% filter(sample_ID %in% contex_real$sample_ID)
# load Halpern data for TARA
load("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/Halpern/halpern_data.Rda")
TARA_contexual_data_1 <- TARA_contexual_data_1 %>% left_join(out_rescaled_2013 %>% rename("sample_ID" = label))
# There should still be 242 samples
TARA_contexual_data_1 %>% dim()
# There should by only 63 prok + SRF samples with Halpern data
TARA_contexual_data_1 %>% select(sample_ID, global_cumul_impact_2013_all_layers_2013_5kms_mean) %>% drop_na() %>% dim()
TARA_contexual_data
# Upload and clean TARA
TARA_contexual_data <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/TARA_metadata_pangaea_ed.txt", col_names = TRUE) %>%
mutate(project = "TARA") %>%
rename(sample_ID = "ena_read_no",
depth_category = "Event",
latitude = latitude_verb,
longitude = longitude_verb,
depth = "depth_verb",
temperature = "Tpot [degreesC]",
salinity = "Sal",
oxygen = "OXYGEN_1 [micromol/kg]",
sampleID_tara = SampleID) %>%
case_when(grepl("0.45-0.8", "0.45-0.8"
"<-0.22", "<-0.22" ))
# Upload and clean TARA
TARA_contexual_data <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/TARA_metadata_pangaea_ed.txt", col_names = TRUE) %>%
mutate(project = "TARA") %>%
rename(sample_ID = "ena_read_no",
depth_category = "Event",
latitude = latitude_verb,
longitude = longitude_verb,
depth = "depth_verb",
temperature = "Tpot [degreesC]",
salinity = "Sal",
oxygen = "OXYGEN_1 [micromol/kg]",
sampleID_tara = SampleID) %>%
case_when(grepl("0.45-0.8", "fuck"
"<-0.22", "you" ))
# Upload and clean TARA
TARA_contexual_data <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/TARA_metadata_pangaea_ed.txt", col_names = TRUE) %>%
mutate(project = "TARA") %>%
rename(sample_ID = "ena_read_no",
depth_category = "Event",
latitude = latitude_verb,
longitude = longitude_verb,
depth = "depth_verb",
temperature = "Tpot [degreesC]",
salinity = "Sal",
oxygen = "OXYGEN_1 [micromol/kg]",
sampleID_tara = SampleID) %>%
case_when(grepl("0.45-0.8", sample_ID) ~ fuck)
# Upload and clean TARA
TARA_contexual_data <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/TARA_metadata_pangaea_ed.txt", col_names = TRUE) %>%
mutate(project = "TARA") %>%
rename(sample_ID = "ena_read_no",
depth_category = "Event",
latitude = latitude_verb,
longitude = longitude_verb,
depth = "depth_verb",
temperature = "Tpot [degreesC]",
salinity = "Sal",
oxygen = "OXYGEN_1 [micromol/kg]",
sampleID_tara = SampleID) %>%
case_when(filter_fraction = grepl("0.45-0.8", sample_ID) ~ fuck)
# Upload and clean TARA
TARA_contexual_data <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/TARA_metadata_pangaea_ed.txt", col_names = TRUE) %>%
mutate(project = "TARA") %>%
rename(sample_ID = "ena_read_no",
depth_category = "Event",
latitude = latitude_verb,
longitude = longitude_verb,
depth = "depth_verb",
temperature = "Tpot [degreesC]",
salinity = "Sal",
oxygen = "OXYGEN_1 [micromol/kg]",
sampleID_tara = SampleID) %>%
mutate(case_when(filter_fraction = grepl("0.45-0.8", sample_ID) ~ fuck))
# Upload and clean TARA
TARA_contexual_data <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/TARA_metadata_pangaea_ed.txt", col_names = TRUE) %>%
mutate(project = "TARA") %>%
rename(sample_ID = "ena_read_no",
depth_category = "Event",
latitude = latitude_verb,
longitude = longitude_verb,
depth = "depth_verb",
temperature = "Tpot [degreesC]",
salinity = "Sal",
oxygen = "OXYGEN_1 [micromol/kg]",
sampleID_tara = SampleID) %>%
mutate(depth_category = case_when(grepl("<-0.22", sample_ID)  ~ 'SRF'))
read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/TARA_metadata_pangaea_ed.txt", col_names = TRUE) %>%
mutate(project = "TARA")
read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/TARA_metadata_pangaea_ed.txt", col_names = TRUE) %>%
mutate(project = "TARA") %>%
rename(sample_ID = "ena_read_no",
depth_category = "Event",
latitude = latitude_verb,
longitude = longitude_verb,
depth = "depth_verb",
temperature = "Tpot [degreesC]",
salinity = "Sal",
oxygen = "OXYGEN_1 [micromol/kg]",
sampleID_tara = SampleID)
read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/TARA_metadata_pangaea_ed.txt", col_names = TRUE) %>%
mutate(project = "TARA") %>%
rename(sample_ID = "ena_read_no",
depth_category = "Event",
latitude = latitude_verb,
longitude = longitude_verb,
depth = "depth_verb",
temperature = "Tpot [degreesC]",
salinity = "Sal",
oxygen = "OXYGEN_1 [micromol/kg]",
sampleID_tara = SampleID) %>%
mutate(filter_fraction = case_when(grepl("<-0.22", sample_ID)    ~ "viral",
grepl("0.1-0.22", sample_ID)  ~ "girus_prok",
grepl("0.22-0.45", sample_ID) ~ "girus_prok",
grepl("0.45-0.8", sample_ID)  ~ "girus_prok",
grepl("0.22-1.6", sample_ID)  ~ "prok",
grepl("0.22-3", sample_ID)    ~ "prok"))
# Upload and clean TARA
TARA_contexual_data <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/TARA_metadata_pangaea_ed.txt", col_names = TRUE) %>%
mutate(project = "TARA") %>%
rename(sample_ID = "ena_read_no",
depth_category = "Event",
latitude = latitude_verb,
longitude = longitude_verb,
depth = "depth_verb",
temperature = "Tpot [degreesC]",
salinity = "Sal",
oxygen = "OXYGEN_1 [micromol/kg]",
sampleID_tara = SampleID) %>%
mutate(filter_fraction = case_when(grepl("<-0.22", sample_ID)    ~ "viral",
grepl("0.1-0.22", sample_ID)  ~ "girus_prok",
grepl("0.22-0.45", sample_ID) ~ "girus_prok",
grepl("0.45-0.8", sample_ID)  ~ "girus_prok",
grepl("0.22-1.6", sample_ID)  ~ "prok",
grepl("0.22-3", sample_ID)    ~ "prok"))
View(TARA_contexual_data)
# Upload and clean TARA
TARA_contexual_data <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/TARA_metadata_pangaea_ed.txt", col_names = TRUE) %>%
mutate(project = "TARA") %>%
rename(sample_ID = "ena_read_no",
depth_category = "Event",
latitude = latitude_verb,
longitude = longitude_verb,
depth = "depth_verb",
temperature = "Tpot [degreesC]",
salinity = "Sal",
oxygen = "OXYGEN_1 [micromol/kg]",
sampleID_tara = SampleID) %>%
mutate(filter_fraction = case_when(grepl("<-0.22", sample_ID)    ~ "viral",
grepl("0.1-0.22", sample_ID)  ~ "girus_prok",
grepl("0.22-0.45", sample_ID) ~ "girus_prok",
grepl("0.45-0.8", sample_ID)  ~ "girus_prok",
grepl("0.22-1.6", sample_ID)  ~ "prok",
grepl("0.22-3", sample_ID)    ~ "prok")) %>%
mutate(depth_category = case_when( grepl("SRF", sample_ID)  ~ 'SRF',
grepl("DCM", sample_ID) ~ 'DCM',
grepl("MES", sample_ID) ~ 'MES'))
TARA_contexual_data %>% select(depth_category)
# Final curated list of samples, use this to compare your results to
contex_real <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/chiaras_samples.txt", col_names = FALSE) %>%
rename(sample_ID = "X1")
# Upload and clean TARA
TARA_contexual_data <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/TARA_metadata_pangaea_ed.txt", col_names = TRUE) %>%
mutate(project = "TARA") %>%
rename(sample_ID = "ena_read_no",
depth_category = "Event",
latitude = latitude_verb,
longitude = longitude_verb,
depth = "depth_verb",
temperature = "Tpot [degreesC]",
salinity = "Sal",
oxygen = "OXYGEN_1 [micromol/kg]",
sampleID_tara = SampleID) %>%
mutate(filter_fraction = case_when(grepl("<-0.22", sample_ID)    ~ "viral",
grepl("0.1-0.22", sample_ID)  ~ "girus_prok",
grepl("0.22-0.45", sample_ID) ~ "girus_prok",
grepl("0.45-0.8", sample_ID)  ~ "girus_prok",
grepl("0.22-1.6", sample_ID)  ~ "prok",
grepl("0.22-3", sample_ID)    ~ "prok")) %>%
mutate(depth_category = case_when( grepl("SRF", sample_ID)  ~ 'SRF',
grepl("DCM", sample_ID) ~ 'DCM',
grepl("MES", sample_ID) ~ 'MES'))
TARA_contexual_data_1 <- TARA_contexual_data %>% filter(sample_ID %in% contex_real$sample_ID)
# load Halpern data for TARA
load("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/Halpern/halpern_data.Rda")
TARA_contexual_data_1 <- TARA_contexual_data_1 %>% left_join(out_rescaled_2013 %>% rename("sample_ID" = label))
# There should still be 242 samples
TARA_contexual_data_1 %>% dim()
# There should by only 63 prok + SRF samples with Halpern data
TARA_contexual_data_1 %>% select(sample_ID, global_cumul_impact_2013_all_layers_2013_5kms_mean) %>% drop_na() %>% dim()
# Upload and clean TARA
TARA_contexual_data <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/TARA_metadata_pangaea_ed.txt", col_names = TRUE) %>%
mutate(project = "TARA") %>%
rename(sample_ID = "ena_read_no",
depth_category = "Event",
latitude = latitude_verb,
longitude = longitude_verb,
depth = "depth_verb",
temperature = "Tpot [degreesC]",
salinity = "Sal",
oxygen = "OXYGEN_1 [micromol/kg]",
sampleID_tara = SampleID) %>%
mutate(filter_fraction = case_when(grepl("<-0.22", sample_ID)    ~ "viral",
grepl("0.1-0.22", sample_ID)  ~ "girus_prok",
grepl("0.22-0.45", sample_ID) ~ "girus_prok",
grepl("0.45-0.8", sample_ID)  ~ "girus_prok",
grepl("0.22-1.6", sample_ID)  ~ "prok",
grepl("0.22-3", sample_ID)    ~ "prok")) %>%
mutate(depth_category = case_when( grepl("SRF", sample_ID)  ~ 'SRF',
grepl("DCM", sample_ID) ~ 'DCM',
grepl("MES", sample_ID) ~ 'MES'))
TARA_contexual_data_1 <- TARA_contexual_data %>% filter(sample_ID %in% contex_real$sample_ID)
# load Halpern data for TARA
load("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/Halpern/halpern_data.Rda")
TARA_contexual_data_1 <- TARA_contexual_data_1 %>% left_join(out_rescaled_2013 %>% rename("sample_ID" = label))
# There should still be 242 samples
TARA_contexual_data_1 %>% dim()
# There should by only 63 prok + SRF samples with Halpern data
TARA_contexual_data_1 %>% select(sample_ID, global_cumul_impact_2013_all_layers_2013_5kms_mean) %>% drop_na() %>% dim()
library(tidyverse)
library(sqldf)
library(RSQLite)
# Final curated list of samples, use this to compare your results to
contex_real <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/chiaras_samples.txt", col_names = FALSE) %>%
rename(sample_ID = "X1")
# upload raw data
#####################################
# GOS data upload
#####################################
# upload raw data
GOS_contexual_data <- read_csv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/CAM_PROJ_GOS.csv", col_names = TRUE) %>%
rename(sample_ID = SAMPLE_NAME, latitude = LATITUDE, longitude = LONGITUDE, depth = SITE_DEPTH, temperature = "temperature - (C)", salinity = "salinity - (psu)")
# Removing sample redundancy from GOS
# take mean of repeated sample data
GOS.numeric <- GOS_contexual_data %>%
group_by(sample_ID) %>%
select_if(is.numeric) %>%
summarise_all(mean)
# only take one string from repeated sample data
GOS.character <- GOS_contexual_data %>%
dplyr::select(-BIOMATERIAL_NAME, -MATERIAL_ACC, -SITE_NAME, -LIBRARY_ACC) %>%
select_if(is.character) %>%
group_by(sample_ID) %>%
unique
# Join two lists of non-redundant GOS data
GOS_contexual_data_1 <- inner_join(GOS.character, GOS.numeric, "sample_ID")
# remove unsequenced samples
GOS_unsequenced <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/GOS_samples_unsequenced.txt", col_names = TRUE) %>%
rename(sample_ID = "label")
# How many unsequenced samples are found in the set?
GOS_unsequenced$label %in% GOS_contexual_data_1$sample_ID %>% length()
# If there are, filter them out
GOS_contexual_data_2 <- GOS_contexual_data_1 %>% anti_join(GOS_unsequenced)
# How many of these samples are found in Chiaras set?
GOS_contexual_data_2$sample_ID %in% contex_real$X1
# If there are remove them
# as Chiara about how to do this
GOS_contexual_data_3 <- GOS_contexual_data_2 %>% anti_join(contex_real %>% filter(grepl("GS", sample_ID)))
GOS_contexual_data_3 <- GOS_contexual_data_2 %>%
mutate(project = "GOS") %>%
filter(!is.na(latitude)) # removed a strange row with a label without data
# Final check to see if your set matches Chiara's set
GOS_contexual_data_3$sample_ID %in% contex_real$sample_ID
#############################################################
#####################################
# Malaspina data upload
#####################################
# Upload
Malaspina_contexual_data <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/Malaspina_Metadata_20170703_ed.txt", col_names = TRUE) %>%
rename(sample_ID = "Code MP####", latitude = Lat, longitude = Long, depth = "DEPTH Megafile", temperature = "Temp", salinity = "Sal (PSU)") %>%
dplyr::select(-Project) %>%
mutate(project = "Malaspina")
Malaspina_header_corrections <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/malaspina_header_corrections.txt", col_names = TRUE) %>% dplyr::select("contex", "orf") %>% drop_na()
# Add "MP" to every sample_ID
Malaspina_contexual_data$sample_ID <- paste0("MP", Malaspina_contexual_data$sample_ID)
# Headers that do not need to be corrected
Malaspina_contexual_data_1 <- Malaspina_contexual_data %>% filter(sample_ID %in% contex_real$sample_ID)
# Headers that need to be changed
Malaspina_contexual_data_2 <- Malaspina_contexual_data %>% filter(!sample_ID %in% contex_real$sample_ID)
# Replace headers that need to be changed with headers that match ORFs
Malaspina_contexual_data_3 <- Malaspina_contexual_data_2 %>% mutate(sample_ID = plyr::mapvalues(x = sample_ID, from = Malaspina_header_corrections$contex, to = Malaspina_header_corrections$orf))
# Bind rows headers that did not need to be changed and headers that were corrected
Malaspina_contexual_data_4 <- bind_rows(Malaspina_contexual_data_1, Malaspina_contexual_data_3)
# Upload and clean TARA
TARA_contexual_data <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/TARA_metadata_pangaea_ed.txt", col_names = TRUE) %>%
mutate(project = "TARA") %>%
rename(sample_ID = "ena_read_no",
depth_category = "Event",
latitude = latitude_verb,
longitude = longitude_verb,
depth = "depth_verb",
temperature = "Tpot [degreesC]",
salinity = "Sal",
oxygen = "OXYGEN_1 [micromol/kg]",
sampleID_tara = SampleID) %>%
mutate(filter_fraction = case_when(grepl("<-0.22", sample_ID)    ~ "viral",
grepl("0.1-0.22", sample_ID)  ~ "girus_prok",
grepl("0.22-0.45", sample_ID) ~ "girus_prok",
grepl("0.45-0.8", sample_ID)  ~ "girus_prok",
grepl("0.22-1.6", sample_ID)  ~ "prok",
grepl("0.22-3", sample_ID)    ~ "prok")) %>%
mutate(depth_category = case_when( grepl("SRF", sample_ID)  ~ 'SRF',
grepl("DCM", sample_ID) ~ 'DCM',
grepl("MES", sample_ID) ~ 'MES'))
TARA_contexual_data_1 <- TARA_contexual_data %>% filter(sample_ID %in% contex_real$sample_ID)
# load Halpern data for TARA
load("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/Halpern/halpern_data.Rda")
TARA_contexual_data_1 <- TARA_contexual_data_1 %>% left_join(out_rescaled_2013 %>% rename("sample_ID" = label))
# There should still be 242 samples
TARA_contexual_data_1 %>% dim()
# There should by only 63 prok + SRF samples with Halpern data
TARA_contexual_data_1 %>% select(sample_ID, global_cumul_impact_2013_all_layers_2013_5kms_mean) %>% drop_na() %>% dim()
# Upload and clean OSD
# These samples have been removed from the OSD data set: (OSD10_2014-06-21_1m_NPL022,
#                                                         OSD18_2014-06-20_75m_NPL022,
#                                                         OSD72_2014-07-21_0.8m_NPL022,
#                                                         OSD96_2014-06-21_0m_NPL022,
#                                                         OSD168_2014-06-21_2m_NPL022)
OSD_contexual_data <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/osd2014_metadata_18-01-2017.tsv", col_names = TRUE) %>%
mutate(project = "OSD") %>%
rename(sample_ID = "label", latitude = start_lat, longitude = start_lon, depth = "water_depth", temperature = "water_temperature", salinity = "salinity")
OSD_header_corrections <- read_tsv("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/OSD_header_corrections.txt", col_names = TRUE) %>% dplyr::select("contex", "orf") %>% drop_na()
OSD_headerstoberemoved <- c("OSD10_2014-06-21_1m_NPL022","OSD18_2014-06-20_75m_NPL022","OSD72_2014-07-21_0.8m_NPL022","OSD96_2014-06-21_0m_NPL022","OSD168_2014-06-21_2m_NPL022")
# there should be a false for each header to be removed
OSD_headerstoberemoved %in% OSD_contexual_data$sample_ID
# Headers that do not need to be changed
OSD_contexual_data_1 <- OSD_contexual_data %>% filter(sample_ID %in% contex_real$sample_ID)
# Headers that do need to be changed
OSD_contexual_data_2 <- OSD_contexual_data %>% filter(!sample_ID %in% contex_real$sample_ID)
# Replace headers that need to be changed with headers that match ORFs
OSD_contexual_data_3 <- OSD_contexual_data_2 %>% mutate(sample_ID = plyr::mapvalues(x = sample_ID, from = OSD_header_corrections$contex, to = OSD_header_corrections$orf))
# Bind rows headers that did not need to be changed and headers that were corrected
OSD_contexual_data_4 <- bind_rows(OSD_contexual_data_1, OSD_contexual_data_3)
# cannot find this... look into it
#OSD_header_corrections <- read_tsv("~/Desktop/msc/msc_git/databases/ORF_headers/OSD_header_correction.txt", col_names = TRUE)
#OSD_contexual_data$sample_ID <- plyr :: mapvalues(OSD_contexual_data$sample_ID, from = OSD_header_corrections$contextual_data, to = OSD_header_corrections$ORF)
############################################################
#chiaras
#OSD_contexual_data_1 <- OSD_contexual_data %>% filter(sample_ID %in% contex_real$sample_ID)
# get ecoregions
library(rgdal)
library(raster)
# for shapefiles, first argument of the read/write/info functions is the
# directory location, and the second is the file name without suffix
# optionally report shapefile details
ogrInfo(dsn = path.expand("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/longhurst_v4_2010/Longhurst_world_v4_2010.shp"), layer = "Longhurst_world_v4_2010")
regions <- readOGR(dsn = path.expand("/Users/mattschechter/Desktop/MarMic/msc_thesis/matt_msc/metadata/longhurst_v4_2010/Longhurst_world_v4_2010.shp"), layer = "Longhurst_world_v4_2010")
#let's see the map
#plot(regions, axes=TRUE, border="gray")
# Function to add Longhurst ecoregion and province
getRegionalInfo  <- function(lat1, long1){
#lat1 <- c(50.09444)
#long1 <- c(-127.5589)
#first, extract the co-ordinates (x,y - i.e., Longitude, Latitude)
coords <- cbind(long1, lat1)
FB.sp <- SpatialPointsDataFrame(coords,data.frame(value = c(4)))
proj4string(FB.sp) <- CRS("+proj=longlat +datum=WGS84 +no_defs")
#plot(regions)
#plot(FB.sp, add=T)
dsdat <- over(regions, FB.sp, add=T, fn = base::mean)
ret <- data.frame(ProvCode = regions$ProvCode[which(dsdat$value==4)],
ProvDescr = regions$ProvDescr[which(dsdat$value==4)])
if(nrow(ret)==0) ret <- data.frame(ProvCode = NA,
ProvDescr = NA)
return(ret)
}
# Name empty lists for each dataset
eco_regions_OSD <- vector(mode = "list")
eco_regions_TARA <- vector(mode = "list")
eco_regions_GOS <- vector(mode = "list")
eco_regions_Malaspina <- vector(mode = "list")
# populate the lists with province and ecoregion
for (i in 1:dim(OSD_contexual_data)[1]){
lat <- OSD_contexual_data[i,]$latitude
lon <- OSD_contexual_data[i,]$longitude
eco_regions_OSD[[i]] <- cbind(OSD_contexual_data[i,]$sample_ID, getRegionalInfo(lat, lon))
}
for (i in 1:dim(TARA_contexual_data_1)[1]){
lat <- TARA_contexual_data_1[i,]$latitude
lon <- TARA_contexual_data_1[i,]$longitude
eco_regions_TARA[[i]] <- cbind(TARA_contexual_data_1[i,]$sample_ID, getRegionalInfo(lat, lon))
}
for (i in 1:dim(GOS_contexual_data_3)[1]){
lat <- GOS_contexual_data_3[i,]$latitude
lon <- GOS_contexual_data_3[i,]$longitude
eco_regions_GOS[[i]] <- cbind(GOS_contexual_data_3[i,]$sample_ID, getRegionalInfo(lat, lon))
}
for (i in 1:dim(Malaspina_contexual_data_4)[1]){
lat <- Malaspina_contexual_data_4[i,]$latitude
lon <- Malaspina_contexual_data_4[i,]$longitude
eco_regions_Malaspina[[i]] <- cbind(Malaspina_contexual_data_4[i,]$sample_ID, getRegionalInfo(lat, lon))
}
# Bind data
# adding Longhurst to OSD
OSD_contexual_data_1 <- bind_rows(eco_regions_OSD) %>%
as_tibble() %>%
rename(label = "OSD_contexual_data[i, ]$sample_ID", ecoregion = ProvCode, province = ProvDescr) %>%
left_join(OSD_contexual_data %>% rename("label" = "sample_ID")) %>%
rename(sample_ID = label)
# adding Longhurst to TARA
TARA_contexual_data_2 <- bind_rows(eco_regions_TARA) %>%
as_tibble() %>%
rename(label = "TARA_contexual_data_1[i, ]$sample_ID", ecoregion = ProvCode, province = ProvDescr) %>%
left_join(TARA_contexual_data_1 %>% rename("label" = "sample_ID")) %>%
rename(sample_ID = label)
# adding Longhurst to Malaspina
Malaspina_contexual_data_5 <- bind_rows(eco_regions_Malaspina) %>%
as_tibble() %>%
rename("label" = "Malaspina_contexual_data_4[i, ]$sample_ID", ecoregion = ProvCode, province = ProvDescr) %>%
left_join(Malaspina_contexual_data_4 %>% rename("label" = "sample_ID")) %>%
rename(sample_ID = label)
# adding Longhurst to GOS
GOS_contexual_data_4 <- bind_rows(eco_regions_GOS) %>%
as_tibble() %>%
rename("label" = "GOS_contexual_data_3[i, ]$sample_ID", ecoregion = ProvCode, province = ProvDescr) %>%
left_join(GOS_contexual_data_3 %>% rename("label" = "sample_ID")) %>%
rename(sample_ID = label)
# extraction not working
# Antonio extracted the data for TARA samples, this was joined in the TARA section above
#================================================================================
#STEP 1: unzip files
#================================================================================
## set dir
setwd("~/Desktop/matt_msc/metadata_1/Halpern/ ")
# extraction not working
# Antonio extracted the data for TARA samples, this was joined in the TARA section above
#================================================================================
#STEP 1: unzip files
#================================================================================
## set dir
#setwd("~/Desktop/matt_msc/metadata_1/Halpern/ ")
## list all files for 2008
files_rescaled_2013 <- list.files(path = "~/Desktop/matt_msc/metadata_1/Halpern/")
print(files_rescaled_2013)
#================================================================================
#STEP 2
#================================================================================
library(proj4)
library(tidyverse)
library(raster)
#my_db<- src_postgres(host = "localhost", port = 5432, dbname = "osd_analysis", options = "-c search_path=osd_analysis")
#osd2014_amp_mg_intersect <- tbl(my_db, "osd2014_amp_mg_intersect") %>%
collect(n = Inf)
<- cbind(long=osd2014_metadata_mg$start_lon, lat=osd2014_metadata_mg$start_lat)                                #(long,lat)
sc <- contexual_data_all %>% filter(grepl("TARA", sample_ID)) %>% dplyr::select(longitude, latitude)
